{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"abcnn.ipynb","version":"0.3.2","provenance":[{"file_id":"1SQgi0PJSDZdFpwU-h0vGRezA2umTTL6B","timestamp":1549289486733}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"ZZV0HVA1hV-M","colab_type":"text"},"cell_type":"markdown","source":["#Initial setup\n","- Tensorboard setup\n","- Google collab and google drive related functions. \n","- GPU and memory consumption setup\n","- NLTK install"]},{"metadata":{"id":"VVm0Ve9Iixph","colab_type":"text"},"cell_type":"markdown","source":["## Tensorboard setup"]},{"metadata":{"id":"c_vM8R3ZHDTg","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","\"\"\"Setup NGROK server for tensorboard\n","   Thanks to question at (https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab) \n","   \n","   Args:1\n","        LOG_DIR (string): Location where the files must be logged\n","\"\"\"\n","  \n","LOG_DIR = '/tmp/log'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","\n","! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","! unzip ngrok-stable-linux-amd64.zip\n","\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"KgNBN3kk2x45","colab_type":"text"},"cell_type":"markdown","source":["## GPU and memory consumption setup"]},{"metadata":{"id":"szyY3ufrM3st","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"Check Google Collab GPU and CPU\n","   Thanks to the question at (https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available)\n","   only one GPU on Colab and it isn't guaranteed. \n","   Prints:\n","        GPU and RAM utilization. \n","\"\"\"\n","\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5NRc8v8yM_Bx","colab_type":"code","colab":{}},"cell_type":"code","source":["GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n","  process = psutil.Process(os.getpid())\n","  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n","  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0OSHCmRS23bY","colab_type":"text"},"cell_type":"markdown","source":["## Google Drive Mount"]},{"metadata":{"id":"WXKRrafhNB2T","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qZb4_2iSNDMn","colab_type":"code","colab":{}},"cell_type":"code","source":["cd /content/gdrive/My Drive/ABCNN/"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aBd5UbdRNEEK","colab_type":"code","colab":{}},"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","!pip3 install beautifultable"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zQM1566I4Z-W","colab_type":"code","colab":{}},"cell_type":"code","source":["ls"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h_btjg1uh-eC","colab_type":"text"},"cell_type":"markdown","source":["# Word2vec init\n","\n","- Instantiates the class and creates an object. (3.6GB file, will take a while)\n"]},{"metadata":{"id":"5D4wB0NcN5a-","colab_type":"code","colab":{}},"cell_type":"code","source":["import gensim\n","import numpy as np\n","class Word2Vec():\n","  \"\"\"Uses gensim to load google's pretrained vectors as the model.\n","  \"\"\"\n","  def __init__(self):\n","    \"\"\"Initialises the class\n","       Attr:\n","        model: A variable holding the embeddings for the words in it's vocabulary. About 3GB size. \n","        unknowns: A variable holding 300 values sampled from a uniform distribution\n","                    from -0.01 to 0.01\n","    \"\"\"\n","    self.model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',\n","                                                                 binary=True)\n","    self.unknowns = np.random.uniform(-0.01, 0.01, 300).astype(\"float32\")\n","\n","  def get(self, word):\n","    \"\"\"Method for returning the keyed vectors for a particular word. \n","       Returns:\n","        If the word is the model's vocab returns the particular embedding else it returns self.unkown holding arbitary embeddings.\n","    \"\"\"\n","    if word not in self.model.vocab:\n","        return self.unknowns\n","    else:\n","        return self.model.word_vec(word)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tzXBPQs1N7lw","colab_type":"code","colab":{}},"cell_type":"code","source":["word2vec = Word2Vec() #Instantiates the class and creates an object. "],"execution_count":0,"outputs":[]},{"metadata":{"id":"77_l8MoLiJr8","colab_type":"text"},"cell_type":"markdown","source":["# Preprocessing for the WikiQA dataset\n","\n","- List of all questions, their corresponding answers and their labels is obtained. \n","- All the questions and the answers are converted into lowercases.\n","- All answers are truncated to max 40 character as per the paper.\n","- List of common words between the question and answer is calculated excluding the common words. This is the word_cnt feature. \n","- List of common words is obtained. Their corresponding Inverse Document frequency (IDF) values is also obtained and summed. This the wgt_word_cnt feature. \n","- Max length is also calculated. This is 40 for our dataset. \n","- Every question and answer is then padded to max_length. (40)\n","\n","Returns the preprocessed questions, answers, labels, features, max_len"]},{"metadata":{"id":"Y0vEivTRN-HY","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"Preprocessing for the WikiQA dataset.\n","   Attr:\n","    max_len: maximum length of the question, answers are truncated to 40 characters. \n","    questions: List of all questions.\n","    answers: List of all answers\n","    labels: 0 or 1 for each question/answer combination\n","    features: list of the following elements for each line: [len(question), len(answer), word_cnt, wgt_word_cnt]\n","      word_cnt: Common words between a question and a answer that are not in the stop words list\n","      wgt_word_cnt: sum of inverse document frequency of all the common words between a question and a answer\n","\"\"\"\n","def preprocess_data(mode):\n","  \n","  if mode == \"train\":\n","    file_path = \"WikiQA_Corpus/WikiQA-train.txt\"\n","  else: \n","    file_path = \"WikiQA_Corpus/WikiQA-test.txt\"\n","    \n","  questions, answers, labels, features = [], [], [], []\n","  stopwords = nltk.corpus.stopwords.words(\"english\") #Load the stopwords from nltk\n","  max_len = 0\n","  vocab = []\n","  idf = {}\n","\n","  with open(file_path,'r',encoding='utf-8') as f:\n","    for line in f:\n","\n","      sentence = line[:-1].split('\\t')\n","\n","      question = sentence[0].lower().split()\n","      answer = sentence[1].lower().split()[:40] #Only 40 words, as per the paper\n","      label = int(sentence[2])\n","\n","      questions.append(question)\n","      answers.append(answer)\n","      labels.append(label)\n","\n","      #Calculate number of common words between the question and answers that are not in the stopwords list.\n","      word_cnt = len([word for word in question if word not in stopwords and word in answer])\n","      features.append([len(question), len(answer), word_cnt])\n","\n","    #Calculate max-length of a sentence in both question and answer\n","    max_len = max(len(max(questions, key=len)),len(max(answers, key=len)))\n","\n","    #Flatten a list and build a vocab using the unique words\n","    vocab = list(set([y for x in questions for y in x]))\n","\n","    #Calculate IDF for every word in vocab:\n","    for w in vocab:\n","      idf[w] = np.log( len(questions)/ len([1 for question in questions for word in question if word==w]) )\n","\n","    #Obtain the common words, calculate the IDF for each word and sum them.\n","    for i in range(len(questions)):\n","      wgt_word_cnt = sum([idf[w] for word in questions[i] if word not in stopwords and word in answers[i]])\n","      features[i].append(wgt_word_cnt)\n","      \n","  return questions, answers, labels, features, max_len"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ICbm36BgOo4X","colab_type":"code","colab":{}},"cell_type":"code","source":["questions, answers, labels, features, max_len = preprocess_data(\"train\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X26jkss5ixlV","colab_type":"text"},"cell_type":"markdown","source":["# Dataloader class.\n","\n","- loads the preprocessed data and provides batch input to the train function\n","\n"]},{"metadata":{"id":"rzIdDLw9O90q","colab_type":"code","colab":{}},"cell_type":"code","source":["class DataLoader():\n","    def __init__(self, word2vec, questions, answers, labels, features, max_len):\n","      \"\"\"This class provides batch input for training and testing.\n","         Attr:\n","            word2vec: word2vec\n","            questions: List of all questions.\n","            answers: List of all answers\n","            labels: 0 or 1 for each question/answer combination\n","            features: list of the following elements for each line: [len(question), len(answer), word_cnt, wgt_word_cnt]\n","            max_len: maximum length of either  the question or the answer. [40]\n","            index: Index to keep track of batches\n","            data_size: size of the data, i.e len(questions)\n","            num_of_features: number of features, 4.\n","      \"\"\"\n","      self.questions, self.answers, self.labels, self.features = questions, answers, labels, features\n","      self.index, self.max_len, self.word2vec = 0, max_len, word2vec\n","      self.data_size = len(self.questions)\n","      self.num_features = len(features[0])\n","    def is_available(self):\n","        if self.index < self.data_size:\n","            return True\n","        else:\n","            return False\n","\n","    def reset_index(self):\n","        self.index = 0\n","\n","    def next(self):\n","        if (self.is_available()):\n","            self.index += 1\n","            return self.data[self.index - 1]\n","        else:\n","            return\n","\n","    def next_batch(self, batch_size):\n","        batch_size = min(self.data_size - self.index, batch_size)\n","        question_mats, answer_mats = [], []\n","\n","        for i in range(batch_size):\n","            question = self.questions[self.index + i]\n","            answer = self.answers[self.index + i]\n","\n","            \"\"\" \n","            • Embedding is obtained for the every word in a question and appended to a list [[]]\n","            • The list of lists is converted into a np.array by np.column_stack [300,number_of_words]\n","            • The column is then padded by the maximum length so that all questions and answers have the same dimension. [300,40]\n","            • The resulting NDarray is then expanded using expand_dims \n","            • Resulting question is of dimension [1,300,40]\n","            \"\"\"\n","            question_mats.append(np.expand_dims(np.pad(np.column_stack([self.word2vec.get(w) for w in question]),\n","                                                 [[0, 0], [0, self.max_len - len(question)]],\n","                                                 \"constant\"), axis=0))\n","\n","            answer_mats.append(np.expand_dims(np.pad(np.column_stack([self.word2vec.get(w) for w in answer]),\n","                                                 [[0, 0], [0, self.max_len - len(answer)]],\n","                                                 \"constant\"), axis=0))\n","\n","        batch_questions = np.concatenate(question_mats, axis=0) #Dimensions[64,300,40]\n","        batch_answers = np.concatenate(answer_mats, axis=0) #Dimensions[64,300,40]\n","        batch_labels = self.labels[self.index:self.index + batch_size] #Dimensions[64]\n","        batch_features = self.features[self.index:self.index + batch_size] #Dimensions[64,4]\n","\n","        self.index += batch_size\n","\n","        return batch_questions, batch_answers, batch_labels, batch_features"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hg1XUAW-PAxi","colab_type":"code","colab":{}},"cell_type":"code","source":["dataloader = DataLoader(word2vec, questions, answers, labels, features, max_len)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k6JDuDmJjdEr","colab_type":"text"},"cell_type":"markdown","source":["# Dataloader Results"]},{"metadata":{"id":"CKnCMZhmPCEO","colab_type":"code","outputId":"f490a270-a1eb-466b-9806-a4094c3e831e","executionInfo":{"status":"ok","timestamp":1549289949993,"user_tz":-330,"elapsed":141000,"user":{"displayName":"ashwin vasudevan","photoUrl":"","userId":"14201573818544118847"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"cell_type":"code","source":["from beautifultable import BeautifulTable\n","\n","dataloader_table = BeautifulTable()\n","\n","dataloader_table.column_headers = [\"Dataloader\", \"Values\"]\n","dataloader_table.append_row([\"Maximum length of Data\", dataloader.max_len])\n","dataloader_table.append_row([\"Number of questions\", len(dataloader.questions)])\n","dataloader_table.append_row([\"Number of answers\", len(dataloader.answers)])\n","dataloader_table.append_row([\"Number of features\", dataloader.num_features])\n","\n","print(dataloader_table)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["+------------------------+--------+\n","|       Dataloader       | Values |\n","+------------------------+--------+\n","| Maximum length of Data |   40   |\n","+------------------------+--------+\n","|  Number of questions   | 20360  |\n","+------------------------+--------+\n","|   Number of answers    | 20360  |\n","+------------------------+--------+\n","|   Number of features   |   4    |\n","+------------------------+--------+\n"],"name":"stdout"}]},{"metadata":{"id":"Z9dExePyjiPO","colab_type":"text"},"cell_type":"markdown","source":["# BCNN\n","\n","- Class definition\n","- Train\n","- Test"]},{"metadata":{"id":"40P5M5f5PHLE","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","class BCNN():\n","    def __init__(self, sentence_length, filter_width, l2_reg, num_features, embedding_dim=300, nb_filters=50, num_classes=2, num_layers=2):\n","        \"\"\"This class implements BCNN arch.\n","           BCNN consists of two CNNs, each processing one of the two sentences, and a final layer that solves the sentence pair tasks. \n","         Attr:\n","            sentence_length: 40\n","            filter_width: 4 width.\n","            l2_reg: 0.0004\n","            num_features: 4\n","            embedding_dim: 300\n","            nb_filters: 50\n","            num_classes: 2\n","            num_layers: 2\n","        \"\"\"\n","\n","        self.x1 = tf.placeholder(tf.float32, shape=[None, embedding_dim, sentence_length], name=\"x1\") #[b,d,s] -> [64,300,40]\n","        self.x2 = tf.placeholder(tf.float32, shape=[None, embedding_dim, sentence_length], name=\"x2\") #[b,d,s] -> #[64,300,40]\n","        self.y = tf.placeholder(tf.int32, shape=[None], name=\"y\") #[b] #64\n","        self.features = tf.placeholder(tf.float32, shape=[None, num_features], name=\"features\") #[b,num_of_filters] -> [64,4] \n","\n","        def pad_for_wide_conv(x):\n","          \"\"\"Zero padding to inputs for wide convolution,\n","            padding w-1 for both sides  (s -> s+w-1)\n","            Attr:\n","                x: input tensor (b, d, s, c) #[64, 300, 40, 1]\n","                w: filter size\n","            Returns:\n","                padded input (b, d, s+w-1 , c) #[64,300,43,1]\n","          \"\"\"\n","          return tf.pad(x, np.array([[0, 0], [0, 0], [filter_width - 1, filter_width - 1], [0, 0]]), \"CONSTANT\", name=\"pad_wide_conv\")\n","\n","        def cos_sim(v1, v2):\n","            \"\"\"Compute the cosine similarity between two vectors v1 and v2\n","               `cosine`: Defined as <x.y>/ |x|*|y|\n","               Args:\n","                v1: vector1\n","                v2: vector2\n","            \"\"\"\n","\n","            norm1 = tf.sqrt(tf.reduce_sum(tf.square(v1), axis=1))\n","            norm2 = tf.sqrt(tf.reduce_sum(tf.square(v2), axis=1))\n","            dot_products = tf.reduce_sum(v1 * v2, axis=1, name=\"cos_sim\")\n","\n","            return dot_products / (norm1 * norm2)\n","   \n","        def convolution(name_scope, x, d, reuse):\n","            \"\"\"conv2D layer\n","               Args:\n","                x: input tensor (b, d, s, c) #[64, 300, 46, 1] if layer 1 or [64,50,46,1]\n","                d: vector2\n","               \n","               Returns:\n","               conv.transpose: (b, d, s, c)[64, 50, 43, 1]\n","               \n","               Namescope is necessary for weight sharing with the second layer\n","            \"\"\"\n","            with tf.name_scope(name_scope + \"-conv\"):\n","                with tf.variable_scope(\"conv\") as scope:\n","                    conv = tf.contrib.layers.conv2d(\n","                        inputs=x,\n","                        num_outputs=nb_filters,\n","                        kernel_size=(d, filter_width),\n","                        stride=1,\n","                        padding=\"VALID\",\n","                        activation_fn=tf.nn.tanh,\n","                        weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n","                        weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_reg),\n","                        biases_initializer=tf.constant_initializer(1e-04),\n","                        reuse=reuse,\n","                        trainable=True,\n","                        scope=scope\n","                    )\n","\n","                    conv_trans = tf.transpose(conv, [0, 3, 2, 1], name=\"conv_trans\") \n","                    return conv_trans\n","\n","        def w_pool(x):\n","            \"\"\"Pooling layer as mentioned in the paper\n","               Args:\n","                x: input tensor (b, d, s, c) #[64, 300, 46, 1] if layer 1 or [64,50,46,1]\n","               \n","               Returns:\n","               conv.transpose: (b, d, s, c)[64, 50, 43, 1]\n","            \"\"\"\n","                \n","            w_ap = tf.layers.average_pooling2d(\n","                inputs=x,\n","                pool_size=(1, filter_width),\n","                strides=1,\n","                padding=\"VALID\",\n","                name=\"w_ap\"\n","            )\n","\n","            return w_ap\n","\n","        def all_pool(variable_scope, x):\n","            \"\"\"Pooling layer as mentioned in the paper\n","               Args:\n","                variable_scope: checks if it's initial inputs\n","                x: input tensor \n","               \n","               Returns:\n","               conv.transpose: (b, d, s, c)[64, 50, 43, 1]\n","            \"\"\"\n","            with tf.variable_scope(variable_scope + \"-all_pool\"):\n","              \n","                if variable_scope.startswith(\"input\"):\n","                    \n","                    pool_width = sentence_length\n","                    d = embedding_dim\n","                else:\n","                    pool_width = sentence_length + filter_width - 1\n","                    d = nb_filters\n","\n","                all_ap = tf.layers.average_pooling2d(\n","                    inputs=x,\n","                    pool_size=(1, pool_width),\n","                    strides=1,\n","                    padding=\"VALID\",\n","                    name=\"all_ap\")\n","\n","                # [batch, di]\n","                all_ap_reshaped = tf.reshape(all_ap, [-1, d])\n","                return all_ap_reshaped\n","\n","        def CNN_layer(variable_scope, x1, x2, d):\n","            \"\"\"Each block contains input -> wide-conv -> w-pool\n","            \"\"\"\n","            with tf.variable_scope(variable_scope):\n","               \n","                left_conv = convolution(name_scope=\"left\", x=pad_for_wide_conv(x1), d=d, reuse=False)\n","                right_conv = convolution(name_scope=\"right\", x=pad_for_wide_conv(x2), d=d, reuse=True)\n","\n","                left_wp = w_pool(x=left_conv)\n","                left_ap = all_pool(variable_scope=\"left\", x=left_conv)\n","                right_wp = w_pool(x=right_conv)\n","                right_ap = all_pool(variable_scope=\"right\", x=right_conv)\n","\n","                return left_wp, left_ap, right_wp, right_ap\n","                    \n","        x1_expanded = tf.expand_dims(self.x1, -1) #[64, 300, 40, 1]\n","        x2_expanded = tf.expand_dims(self.x2, -1)  #[64, 300, 40, 1]\n","\n","        LO_0 = all_pool(variable_scope=\"input-left\", x=x1_expanded)  #[64, 300, 1, 1]\n","        RO_0 = all_pool(variable_scope=\"input-right\", x=x2_expanded) #[64, 300, 1, 1]\n","\n","        LI_1, LO_1, RI_1, RO_1 = CNN_layer(variable_scope=\"CNN-1\", x1=x1_expanded, x2=x2_expanded, d=embedding_dim)\n","        sims = [cos_sim(LO_0, RO_0), cos_sim(LO_1, RO_1)] #Compute similarity scores and store them.\n","\n","        if num_layers > 1:\n","            \"\"\" Create second CNN block if num_layers > 1\n","                Output from the first layer is given as input to the second\n","            \"\"\"\n","            _, LO_2, _, RO_2 = CNN_layer(variable_scope=\"CNN-2\", x1=LI_1, x2=RI_1, d=nb_filters)\n","            sims.append(cos_sim(LO_2, RO_2)) # Compute similarity scores for the second block too. \n","\n","        with tf.variable_scope(\"output-layer\"):\n","            \"\"\" Final Output layer\"\"\"            \n","            self.output_features = tf.concat([self.features, tf.stack(sims, axis=1)], axis=1, name=\"output_features\")\n","\n","            self.estimation = tf.contrib.layers.fully_connected(\n","                inputs=self.output_features,\n","                num_outputs=num_classes,\n","                activation_fn=None,\n","                weights_initializer=tf.contrib.layers.xavier_initializer(),\n","                weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_reg),\n","                biases_initializer=tf.constant_initializer(1e-04),\n","                scope=\"FC\"\n","            )\n","\n","        self.prediction = tf.contrib.layers.softmax(self.estimation)[:, 1]\n","      \n","        \"\"\" Calculate cost by softmax_cross_entropy and add a regularizer term \"\"\"\n","        self.cost = tf.add(\n","            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.estimation, labels=self.y)),\n","            tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)),\n","            name=\"cost\")\n","        \n","        tf.summary.scalar(\"cost\", self.cost)\n","        self.merged = tf.summary.merge_all()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I8_ZFtKqjmdG","colab_type":"text"},"cell_type":"markdown","source":["## Train"]},{"metadata":{"id":"oVGdRKexVSet","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import sys\n","\n","from sklearn import linear_model\n","from sklearn.externals import joblib\n","\n","\n","\n","def build_path(prefix, model_type, num_layers, postpix=\"\"):\n","    return prefix + \"-\" + model_type + \"-\" + str(num_layers) + postpix\n","\n","\n","def train(learning_rate, filter_width, l2_reg, nb_epoch, batch_size, model_type, num_layers, word2vec, embedding_dim, nb_filters, num_classes=2):\n","    \"\"\" Reset Default graph to rerun and test the cell multiple times\"\"\"\n","    tf.reset_default_graph()\n","\n","    \n","    if model_type == \"BCNN\":\n","      model = BCNN(sentence_length=dataloader.max_len, filter_width=filter_width, l2_reg=l2_reg,\n","                  num_features=dataloader.num_features, num_classes=num_classes, num_layers=num_layers, embedding_dim=embedding_dim, nb_filters=nb_filters)\n","    else: \n","      \n","      model = ABCNN(sentence_length=dataloader.max_len, filter_width=filter_width, l2_reg=l2_reg,\n","                  num_features=dataloader.num_features, num_classes=num_classes, num_layers=num_layers, embedding_dim=embedding_dim, nb_filters=nb_filters)\n","      \n","    optimizer = tf.train.AdagradOptimizer(learning_rate, name=\"optimizer\").minimize(model.cost)\n","\n","    init = tf.global_variables_initializer()\n","    saver = tf.train.Saver(max_to_keep=100)\n","    \n","    \n","\n","    with tf.device(\"/device:GPU:0\"):\n","      with tf.Session() as sess:\n","          train_summary_writer = tf.summary.FileWriter(\"/tmp/log/\", sess.graph)\n","\n","          sess.run(init)\n","          \n","          print(\"=\" * 70)\n","          for e in range(1, nb_epoch + 1):\n","            \n","              epoch_table = BeautifulTable()\n","              epoch_table.column_headers = [\"Epoch \" + str(e) + \"/\" + str(nb_epoch)]\n","                            \n","              dataloader.reset_index()\n","              i = 0\n","\n","              LR = linear_model.LogisticRegression(solver='lbfgs')\n","              clf_features = []\n","\n","              epoch_loss = 0\n","              \n","              while dataloader.is_available():\n","                  i += 1\n","\n","                  batch_x1, batch_x2, batch_y, batch_features = dataloader.next_batch(batch_size=batch_size)\n","\n","                  merged, _, c, features = sess.run([model.merged, optimizer, model.cost, model.output_features],\n","                                                    feed_dict={model.x1: batch_x1,\n","                                                               model.x2: batch_x2,\n","                                                               model.y: batch_y,\n","                                                               model.features: batch_features})\n","                  \n","                  clf_features.append(features)\n","                  epoch_loss += c\n","\n","              epoch_table.append_row([\"Cost: \" + str(epoch_loss)])\n","              print(epoch_table)\n","              \n","              cost_path = build_path(\"./cost/\", model_type, num_layers, \".txt\")\n","              with open(cost_path, 'a') as f:\n","                f.write(str(epoch_loss) + \"\\n\")\n","              \n","              #train_summary_writer.add_summary(merged, i)\n","              \n","              save_path = saver.save(sess, build_path(\"./models/\", model_type, num_layers), global_step=e)\n","              \"\"\" Performance increases if we do not use the output of the LR layer as the \n","              final decision, but instead train a linear SVM or a logistic regression with default parameters2 directly\n","              on the input to the LR layer. (From the paper)\"\"\"\n","              clf_features = np.concatenate(clf_features)\n","              LR.fit(clf_features, dataloader.labels)\n","\n","              LR_path = build_path(\"./models/\", model_type, num_layers, \"-\" + str(e) + \"-LR.pkl\")\n","              joblib.dump(LR, LR_path)\n","\n","          \n","          print(\"training finished!\")\n","          print(\"=\" * 50)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2NaN-8w2VaUa","colab_type":"code","colab":{}},"cell_type":"code","source":["param_table = BeautifulTable()\n","param_table.column_headers = [\"Parameter\", \"Value\"]\n","\n","params = {\n","    \"learning_rate\": 0.08,\n","    \"filter_width\": 4,\n","    \"l2_reg\": 0.0004,\n","    \"nb_epoch\": 50,\n","    \"batch_size\": 64,\n","    \"model_type\": \"BCNN\",\n","    \"num_layers\": 2,\n","    \"word2vec\": word2vec,\n","    \"embedding_dim\": 300,\n","    \"nb_filters\": 50\n","}\n","\n","for k,v in params.items():\n","  param_table.append_row([k,v])\n","print(param_table)\n","\n","\n","train(learning_rate=float(params[\"learning_rate\"]), filter_width=int(params[\"filter_width\"]), l2_reg=float(params[\"l2_reg\"]), nb_epoch=int(params[\"nb_epoch\"]),\n","      batch_size=int(params[\"batch_size\"]), model_type=params[\"model_type\"], num_layers=int(params[\"num_layers\"]),\n","      word2vec=params[\"word2vec\"],embedding_dim=int(params[\"embedding_dim\"]),nb_filters=int(params[\"nb_filters\"]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"djp4nQsdj66B","colab_type":"text"},"cell_type":"markdown","source":["## Process and load Test Dataset"]},{"metadata":{"id":"RBxdKe4YZhk0","colab_type":"code","colab":{}},"cell_type":"code","source":["questions, answers, labels, features, max_len = preprocess_data(\"test\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PNGtLq6HZtl2","colab_type":"code","colab":{}},"cell_type":"code","source":["dataloader = DataLoader(word2vec, questions, answers, labels, features, max_len)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xkzkKTBrZuPl","colab_type":"code","colab":{}},"cell_type":"code","source":["from beautifultable import BeautifulTable\n","\n","dataloader_table = BeautifulTable()\n","\n","dataloader_table.column_headers = [\"Dataloader\", \"Values\"]\n","dataloader_table.append_row([\"Maximum length of Data\", dataloader.max_len])\n","dataloader_table.append_row([\"Number of questions\", len(dataloader.questions)])\n","dataloader_table.append_row([\"Number of answers\", len(dataloader.answers)])\n","dataloader_table.append_row([\"Number of features\", dataloader.num_features])\n","\n","print(dataloader_table)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DtSVcbmEke6u","colab_type":"text"},"cell_type":"markdown","source":["## Run test"]},{"metadata":{"id":"bf1_0cy9YqRt","colab_type":"code","colab":{}},"cell_type":"code","source":["def test(filter_width, l2_reg, nb_epoch, model_type, num_layers, classifier, word2vec, num_classes=2):\n","  \n","    \"\"\" Reset Default graph to rerun and test the cell multiple times\"\"\"\n","    tf.reset_default_graph()\n","\n","    if model_type == \"BCNN\":\n","      model = BCNN(sentence_length=dataloader.max_len, filter_width=filter_width, l2_reg=l2_reg,\n","                    num_features=dataloader.num_features, num_classes=num_classes, num_layers=num_layers)\n","    else:\n","      model = ABCNN(sentence_length=dataloader.max_len, filter_width=filter_width, l2_reg=l2_reg,\n","                    num_features=dataloader.num_features, num_classes=num_classes, num_layers=num_layers)\n","    \n","    \n","    model_path = build_path(\"./models/\", model_type, num_layers)\n","    MAPs, MRRs = [], []\n","\n","    print(\"=\" * 50)\n","    print(\"test data size:\", dataloader.data_size)\n","\n","    for e in range(1, nb_epoch + 1):\n","      \n","      epoch_table = BeautifulTable()\n","      epoch_table.column_headers = [\"Epoch \" + str(e) + \"/\" + str(50)]\n","      dataloader.reset_index()\n","        \n","      #with tf.device(\"/device:GPU:0\"): #TF bug, can't assigned GPU but it runs on GPU verified\n","      with tf.Session() as sess:\n","          saver = tf.train.Saver()\n","          saver.restore(sess, model_path + \"-\" + str(e))\n","          print(model_path + \"-\" + str(e), \"restored.\")\n","\n","          if classifier == \"LR\":\n","              clf_path = build_path(\"./models/\", model_type, num_layers,\n","                                    \"-\" + str(e) + \"-\" + classifier + \".pkl\")\n","              clf = joblib.load(clf_path)\n","\n","\n","          QA_pairs = {}\n","          questions, answers, labels, features = dataloader.next_batch(batch_size=dataloader.data_size)\n","\n","          for i in range(dataloader.data_size):\n","              pred, clf_input = sess.run([model.prediction, model.output_features],\n","                                         feed_dict={model.x1: np.expand_dims(questions[i], axis=0),\n","                                                    model.x2: np.expand_dims(answers[i], axis=0),\n","                                                    model.y: np.expand_dims(labels[i], axis=0),\n","                                                    model.features: np.expand_dims(features[i], axis=0)})\n","\n","              if classifier == \"LR\":\n","                  clf_pred = clf.predict_proba(clf_input)[:, 1]\n","                  pred = clf_pred\n","\n","              question = \" \".join(dataloader.questions[i])\n","              answer = \" \".join(dataloader.answers[i])\n","\n","              if question in QA_pairs:\n","                  QA_pairs[question].append((answer, labels[i], np.asscalar(pred)))\n","              else:\n","                  QA_pairs[question] = [(answer, labels[i], np.asscalar(pred))]\n","\n","          \"\"\"\n","          Calculate MAP and MRR for each saved model.\n","          \"\"\"\n","          MAP, MRR = 0, 0\n","          for s1 in QA_pairs.keys():\n","              p, AP = 0, 0\n","              MRR_check = False\n","\n","              QA_pairs[s1] = sorted(QA_pairs[s1], key=lambda x: x[-1], reverse=True)\n","\n","              for idx, (s2, label, prob) in enumerate(QA_pairs[s1]):\n","                  if label == 1:\n","                      if not MRR_check:\n","                          MRR += 1 / (idx + 1)\n","                          MRR_check = True\n","\n","                      p += 1\n","                      AP += p / (idx + 1)\n","\n","              AP /= p\n","              MAP += AP\n","\n","          num_questions = len(QA_pairs.keys())\n","          MAP /= num_questions\n","          MRR /= num_questions\n","\n","          epoch_table.append_row([\"MAP: \" + str(MAP)])\n","          epoch_table.append_row([\"MRR: \" + str(MRR)])\n","          print(epoch_table)\n","\n","          MAPs.append(MAP)\n","          MRRs.append(MRR)\n","\n","    print(\"=\" * 50)\n","    print(\"max MAP:\", max(MAPs), \"max MRR:\", max(MRRs))\n","    print(\"=\" * 50)\n","    \n","    \"\"\" Write the MAP and MMR for each corresponding epoch to a file.\"\"\"\n","\n","    exp_path = build_path(\"./experiments/\", model_type, num_layers, \"-\" + classifier + \".txt\")\n","    with open(exp_path, \"w\", encoding=\"utf-8\") as f:\n","        print(\"Epoch\\tMAP\\tMRR\", file=f)\n","        for i in range(e):\n","            print(str(i + 1) + \"\\t\" + str(MAPs[i]) + \"\\t\" + str(MRRs[i]), file=f)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7hMBntWxbE-b","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","\n","# default parameters\n","params = {\n","    \"filter_width\": 4,\n","    \"l2_reg\": 0.0004,\n","    \"nb_epoch\": 50,\n","    \"model_type\": \"BCNN\",\n","    \"num_layers\": 2,\n","    \"classifier\": \"LR\",\n","    \"word2vec\": word2vec\n","}\n","\n","\n","test(filter_width=int(params[\"filter_width\"]), l2_reg=float(params[\"l2_reg\"]), nb_epoch=int(params[\"nb_epoch\"])\n","     , model_type=params[\"model_type\"], num_layers=int(params[\"num_layers\"]), classifier=params[\"classifier\"],\n","     word2vec=params[\"word2vec\"])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mJ5J61AJySWm","colab_type":"text"},"cell_type":"markdown","source":["## Cost and MAP,MMR graph"]},{"metadata":{"id":"fMUnr_-RbiLI","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","plt.style.use('ggplot')\n","\n","def plt_graphs(cost_file,score_file,position):\n","    with open(cost_file,'r') as f:\n","        values = f.read().split(\"\\n\")\n","\n","    cost = values[:-1]\n","    cost[:] = [float(x) for x in cost]\n","\n","    with open(score_file,'r') as f:\n","        values = f.read().split('\\n')\n","    values = values[:-1]\n","\n","\n","    maps = [x.split('\\t')[1] for x in values][1:]\n","    mmrs = [x.split('\\t')[2] for x in values][1:]\n","    maps[:] = [float(x) for x in maps]\n","    mmrs[:] = [float(x) for x in mmrs]\n","\n","    summed_maps_mmrs = [sum(x) for x in zip(maps,mmrs)]\n","    max_score = max(summed_maps_mmrs)\n","    max_score_index = summed_maps_mmrs.index(max_score)\n","\n","    x = np.arange(1,51,1)\n","    y = maps\n","    y2 = mmrs\n","    y3 = cost\n","\n","    fig = plt.figure(figsize=(12,9))\n","    ax1 = fig.add_subplot(1,1,1)\n","    ax1.plot(x,y,'b',label=\"MAP score\")\n","    ax1.plot(x,y2,'g', label=\"MMR score\")\n","\n","\n","    ax1.set_title(\"MAP, MMR and Loss over epochs\")\n","    ax1.set_xlabel(\"Epochs\")\n","    ax1.set_ylabel(\"MMR and MAP scores\")\n","\n","    ax2 = ax1.twinx() \n","    ax2.set_ylabel(\"Cost function\")\n","    ax2.plot(x,y3, label=\"Loss\")\n","\n","    ax1.legend(loc=\"upper right\",  prop={'size': 15}, bbox_to_anchor=(1, 0.9))\n","    ax2.legend(loc=\"upper right\", prop={'size':15})\n","\n","    string_score =\"Max MAPS = {:.4f}\\nMax MMR = {:.4f}\\nEpoch = {}\".format(maps[max_score_index],mmrs[max_score_index],max_score_index)\n","    fig.text(position[0],position[1],string_score,bbox={'boxstyle':'square',\"color\":\"white\"},fontdict={'color':'black','size':'15'})\n","    \n","    fig.savefig('BCNN-2-Trec.png')\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KqBMyxR5x39i","colab_type":"code","colab":{}},"cell_type":"code","source":["cost_file = \"cost/-BCNN-2.txt\"\n","score_file = \"experiments/-BCNN-2-LR.txt\"\n","position = [0.5,0.75]\n","plt_graphs(cost_file,score_file,position)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sS4jlXWBj5rg","colab_type":"text"},"cell_type":"markdown","source":["# ABCNN\n","\n","Same class as BCNN but only one extra function added to compute the attention matrix"]},{"metadata":{"id":"J43na79wj_zd","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","class ABCNN():\n","    def __init__(self, sentence_length, filter_width, l2_reg, num_features, embedding_dim=300, nb_filters=50, num_classes=2, num_layers=2):\n","        \"\"\"This class implements ABCNN arch. \n","         Attr:\n","            sentence_length: 40\n","            filter_width: 4 width.\n","            l2_reg: 0.0004\n","            num_features: 4\n","            embedding_dim: 300\n","            nb_filters: 50\n","            num_classes: 2\n","            num_layers: 2\n","        \"\"\"\n","\n","        self.x1 = tf.placeholder(tf.float32, shape=[None, embedding_dim, sentence_length], name=\"x1\") #[b,d,s] -> [64,300,40]\n","        self.x2 = tf.placeholder(tf.float32, shape=[None, embedding_dim, sentence_length], name=\"x2\") #[b,d,s] -> #[64,300,40]\n","        self.y = tf.placeholder(tf.int32, shape=[None], name=\"y\") #[b] #64\n","        self.features = tf.placeholder(tf.float32, shape=[None, num_features], name=\"features\") #[b,num_of_filters] -> [64,4] \n","\n","        def pad_for_wide_conv(x):\n","          \"\"\"Zero padding to inputs for wide convolution,\n","            padding w-1 for both sides  (s -> s+w-1)\n","            Attr:\n","                x: input tensor (b, d, s, c) #[64, 300, 40, 1]\n","                w: filter size\n","            Returns:\n","                padded input (b, d, s+w-1 , c) #[64,300,43,1]\n","          \"\"\"\n","          return tf.pad(x, np.array([[0, 0], [0, 0], [filter_width - 1, filter_width - 1], [0, 0]]), \"CONSTANT\", name=\"pad_wide_conv\")\n","\n","        def cos_sim(v1, v2):\n","            \"\"\"Compute the cosine similarity between two vectors v1 and v2\n","               `cosine`: Defined as <x.y>/ |x|*|y|\n","               Args:\n","                v1: vector1\n","                v2: vector2\n","            \"\"\"\n","\n","            norm1 = tf.sqrt(tf.reduce_sum(tf.square(v1), axis=1))\n","            norm2 = tf.sqrt(tf.reduce_sum(tf.square(v2), axis=1))\n","            dot_products = tf.reduce_sum(v1 * v2, axis=1, name=\"cos_sim\")\n","\n","            return dot_products / (norm1 * norm2)\n","        \n","        def make_attention_mat(x1, x2):\n","            \n","            euclidean = tf.sqrt(tf.reduce_sum(tf.square(x1 - tf.matrix_transpose(x2)), axis=1))\n","            return 1 / (1 + euclidean)\n","   \n","        def convolution(name_scope, x, d, reuse):\n","            \"\"\"conv2D layer\n","               Args:\n","                x: input tensor (b, d, s, c) #[64, 300, 46, 1] if layer 1 or [64,50,46,1]\n","                d: vector2\n","               \n","               Returns:\n","               conv.transpose: (b, d, s, c)[64, 50, 43, 1]\n","               \n","               Namescope is necessary for weight sharing with the second layer\n","            \"\"\"\n","            with tf.name_scope(name_scope + \"-conv\"):\n","                with tf.variable_scope(\"conv\") as scope:\n","                    conv = tf.contrib.layers.conv2d(\n","                        inputs=x,\n","                        num_outputs=nb_filters,\n","                        kernel_size=(d, filter_width),\n","                        stride=1,\n","                        padding=\"VALID\",\n","                        activation_fn=tf.nn.tanh,\n","                        weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n","                        weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_reg),\n","                        biases_initializer=tf.constant_initializer(1e-04),\n","                        reuse=reuse,\n","                        trainable=True,\n","                        scope=scope\n","                    )\n","\n","                    conv_trans = tf.transpose(conv, [0, 3, 2, 1], name=\"conv_trans\") \n","                    return conv_trans\n","\n","        def w_pool(x):\n","            \"\"\"Pooling layer as mentioned in the paper\n","               Args:\n","                x: input tensor (b, d, s, c) #[64, 300, 46, 1] if layer 1 or [64,50,46,1]\n","               \n","               Returns:\n","               conv.transpose: (b, d, s, c)[64, 50, 43, 1]\n","            \"\"\"\n","                \n","            w_ap = tf.layers.average_pooling2d(\n","                inputs=x,\n","                pool_size=(1, filter_width),\n","                strides=1,\n","                padding=\"VALID\",\n","                name=\"w_ap\"\n","            )\n","\n","            return w_ap\n","\n","        def all_pool(variable_scope, x):\n","            \"\"\"Pooling layer as mentioned in the paper\n","               Args:\n","                variable_scope: checks if it's initial inputs\n","                x: input tensor \n","               \n","               Returns:\n","               conv.transpose: (b, d, s, c)[64, 50, 43, 1]\n","            \"\"\"\n","            with tf.variable_scope(variable_scope + \"-all_pool\"):\n","              \n","                if variable_scope.startswith(\"input\"):\n","                    \n","                    pool_width = sentence_length\n","                    d = embedding_dim\n","                else:\n","                    pool_width = sentence_length + filter_width - 1\n","                    d = nb_filters\n","\n","                all_ap = tf.layers.average_pooling2d(\n","                    inputs=x,\n","                    pool_size=(1, pool_width),\n","                    strides=1,\n","                    padding=\"VALID\",\n","                    name=\"all_ap\")\n","\n","                # [batch, di]\n","                all_ap_reshaped = tf.reshape(all_ap, [-1, d])\n","                return all_ap_reshaped\n","\n","        def CNN_layer(variable_scope, x1, x2, d):\n","            \"\"\"Each block contains input -> wide-conv -> w-pool\n","            \"\"\"\n","            with tf.variable_scope(variable_scope):\n","              \n","              with tf.name_scope(\"att_mat\"):\n","                aW = tf.get_variable(name=\"aW\",\n","                                     shape=(sentence_length, d),\n","                                     initializer=tf.contrib.layers.xavier_initializer(),\n","                                     regularizer=tf.contrib.layers.l2_regularizer(scale=l2_reg))\n","\n","                \"\"\" Compute attention matrix\"\"\"\n","                att_mat = make_attention_mat(x1, x2)\n","\n","                \"\"\" Transform into the same dimension as the input\"\"\"\n","                x1_a = tf.expand_dims(tf.matrix_transpose(tf.einsum(\"ijk,kl->ijl\", att_mat, aW)), -1)\n","                x2_a = tf.expand_dims(tf.matrix_transpose(\n","                    tf.einsum(\"ijk,kl->ijl\", tf.matrix_transpose(att_mat), aW)), -1)\n","\n","                \"\"\"Concat with input\"\"\"\n","                x1 = tf.concat([x1, x1_a], axis=3)\n","                x2 = tf.concat([x2, x2_a], axis=3)\n","               \n","              left_conv = convolution(name_scope=\"left\", x=pad_for_wide_conv(x1), d=d, reuse=False)\n","              right_conv = convolution(name_scope=\"right\", x=pad_for_wide_conv(x2), d=d, reuse=True)\n","\n","              left_wp = w_pool(x=left_conv)\n","              left_ap = all_pool(variable_scope=\"left\", x=left_conv)\n","              right_wp = w_pool(x=right_conv)\n","              right_ap = all_pool(variable_scope=\"right\", x=right_conv)\n","\n","              return left_wp, left_ap, right_wp, right_ap\n","                    \n","        x1_expanded = tf.expand_dims(self.x1, -1) #[64, 300, 40, 1]\n","        x2_expanded = tf.expand_dims(self.x2, -1)  #[64, 300, 40, 1]\n","\n","        LO_0 = all_pool(variable_scope=\"input-left\", x=x1_expanded)  #[64, 300, 1, 1]\n","        RO_0 = all_pool(variable_scope=\"input-right\", x=x2_expanded) #[64, 300, 1, 1]\n","\n","        LI_1, LO_1, RI_1, RO_1 = CNN_layer(variable_scope=\"CNN-1\", x1=x1_expanded, x2=x2_expanded, d=embedding_dim)\n","        sims = [cos_sim(LO_0, RO_0), cos_sim(LO_1, RO_1)] #Compute similarity scores and store them.\n","\n","        if num_layers > 1:\n","            \"\"\" Create second CNN block if num_layers > 1\n","                Output from the first layer is given as input to the second\n","            \"\"\"\n","            _, LO_2, _, RO_2 = CNN_layer(variable_scope=\"CNN-2\", x1=LI_1, x2=RI_1, d=nb_filters)\n","            sims.append(cos_sim(LO_2, RO_2)) # Compute similarity scores for the second block too. \n","\n","        with tf.variable_scope(\"output-layer\"):\n","            \"\"\" Final Output layer\"\"\"            \n","            self.output_features = tf.concat([self.features, tf.stack(sims, axis=1)], axis=1, name=\"output_features\")\n","\n","            self.estimation = tf.contrib.layers.fully_connected(\n","                inputs=self.output_features,\n","                num_outputs=num_classes,\n","                activation_fn=None,\n","                weights_initializer=tf.contrib.layers.xavier_initializer(),\n","                weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_reg),\n","                biases_initializer=tf.constant_initializer(1e-04),\n","                scope=\"FC\"\n","            )\n","\n","        self.prediction = tf.contrib.layers.softmax(self.estimation)[:, 1]\n","      \n","        \"\"\" Calculate cost by softmax_cross_entropy and add a regularizer term \"\"\"\n","        self.cost = tf.add(\n","            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.estimation, labels=self.y)),\n","            tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)),\n","            name=\"cost\")\n","        \n","        tf.summary.scalar(\"cost\", self.cost)\n","        self.merged = tf.summary.merge_all()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hrgHtxP7510N","colab_type":"text"},"cell_type":"markdown","source":["## Train"]},{"metadata":{"id":"da5x_bkstCJE","colab_type":"code","colab":{}},"cell_type":"code","source":["questions, answers, labels, features, max_len = preprocess_data(\"train\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FA2scN7KtFjD","colab_type":"code","colab":{}},"cell_type":"code","source":["dataloader = DataLoader(word2vec, questions, answers, labels, features, max_len)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UIqceoWOtErV","colab_type":"code","colab":{}},"cell_type":"code","source":["from beautifultable import BeautifulTable\n","\n","dataloader_table = BeautifulTable()\n","\n","dataloader_table.column_headers = [\"Dataloader\", \"Values\"]\n","dataloader_table.append_row([\"Maximum length of Data\", dataloader.max_len])\n","dataloader_table.append_row([\"Number of questions\", len(dataloader.questions)])\n","dataloader_table.append_row([\"Number of answers\", len(dataloader.answers)])\n","dataloader_table.append_row([\"Number of features\", dataloader.num_features])\n","\n","print(dataloader_table)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x0Wo7JNMtJnk","colab_type":"code","colab":{}},"cell_type":"code","source":["param_table = BeautifulTable()\n","param_table.column_headers = [\"Parameter\", \"Value\"]\n","\n","params = {\n","    \"learning_rate\": 0.085,\n","    \"filter_width\": 4,\n","    \"l2_reg\": 0.0006,\n","    \"nb_epoch\": 50,\n","    \"batch_size\": 64,\n","    \"model_type\": \"ABCNN1\",\n","    \"num_layers\": 2,\n","    \"word2vec\": word2vec,\n","    \"embedding_dim\": 300,\n","    \"nb_filters\": 50\n","    \n","}\n","\n","for k,v in params.items():\n","  param_table.append_row([k,v])\n","print(param_table)\n","\n","\n","train(learning_rate=float(params[\"learning_rate\"]), filter_width=int(params[\"filter_width\"]), l2_reg=float(params[\"l2_reg\"]), nb_epoch=int(params[\"nb_epoch\"]),\n","      batch_size=int(params[\"batch_size\"]), model_type=params[\"model_type\"], num_layers=int(params[\"num_layers\"]),\n","      word2vec=params[\"word2vec\"],embedding_dim=int(params[\"embedding_dim\"]),nb_filters=int(params[\"nb_filters\"]))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"UnAZ1lFD1NeG"},"cell_type":"markdown","source":["## Process and load Test Dataset"]},{"metadata":{"colab_type":"code","id":"DggcI7us1NeI","colab":{}},"cell_type":"code","source":["questions, answers, labels, features, max_len = preprocess_data(\"test\")"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"YhOaw-Si1NeR","colab":{}},"cell_type":"code","source":["dataloader = DataLoader(word2vec, questions, answers, labels, features, max_len)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"PZKoFfYq1NeU","colab":{}},"cell_type":"code","source":["from beautifultable import BeautifulTable\n","\n","dataloader_table = BeautifulTable()\n","\n","dataloader_table.column_headers = [\"Dataloader\", \"Values\"]\n","dataloader_table.append_row([\"Maximum length of Data\", dataloader.max_len])\n","dataloader_table.append_row([\"Number of questions\", len(dataloader.questions)])\n","dataloader_table.append_row([\"Number of answers\", len(dataloader.answers)])\n","dataloader_table.append_row([\"Number of features\", dataloader.num_features])\n","\n","print(dataloader_table)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"A-9HIUCy1Vwm"},"cell_type":"markdown","source":["## Run test"]},{"metadata":{"colab_type":"code","id":"xmmv6V9k1Vwo","colab":{}},"cell_type":"code","source":["\n","# default parameters\n","params = {\n","    \"filter_width\": 4,\n","    \"l2_reg\": 0.0006,\n","    \"nb_epoch\": 50,\n","    \"model_type\": \"ABCNN1\",\n","    \"num_layers\": 2,\n","    \"classifier\": \"LR\",\n","    \"word2vec\": word2vec\n","}\n","\n","\n","test(filter_width=int(params[\"filter_width\"]), l2_reg=float(params[\"l2_reg\"]), nb_epoch=int(params[\"nb_epoch\"])\n","     , model_type=params[\"model_type\"], num_layers=int(params[\"num_layers\"]), classifier=params[\"classifier\"],\n","     word2vec=params[\"word2vec\"])"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"4yVzoLev1lBq"},"cell_type":"markdown","source":["## Cost and MAP,MMR graph"]},{"metadata":{"colab_type":"code","id":"N5wnO0qH1lBs","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","plt.style.use('ggplot')\n","\n","def plt_graphs(cost_file,score_file,position):\n","    with open(cost_file,'r') as f:\n","        values = f.read().split(\"\\n\")\n","\n","    cost = values[:-1]\n","    cost[:] = [float(x) for x in cost]\n","\n","    with open(score_file,'r') as f:\n","        values = f.read().split('\\n')\n","    values = values[:-1]\n","\n","\n","    maps = [x.split('\\t')[1] for x in values][1:]\n","    mmrs = [x.split('\\t')[2] for x in values][1:]\n","    maps[:] = [float(x) for x in maps]\n","    mmrs[:] = [float(x) for x in mmrs]\n","\n","    summed_maps_mmrs = [sum(x) for x in zip(maps,mmrs)]\n","    max_score = max(summed_maps_mmrs)\n","    max_score_index = summed_maps_mmrs.index(max_score)\n","\n","    x = np.arange(1,51,1)\n","    y = maps\n","    y2 = mmrs\n","    y3 = cost\n","\n","    fig = plt.figure(figsize=(12,9))\n","    ax1 = fig.add_subplot(1,1,1)\n","    ax1.plot(x,y,'b',label=\"MAP score\")\n","    ax1.plot(x,y2,'g', label=\"MMR score\")\n","\n","\n","    ax1.set_title(\"MAP, MMR and Loss over epochs\")\n","    ax1.set_xlabel(\"Epochs\")\n","    ax1.set_ylabel(\"MMR and MAP scores\")\n","\n","    ax2 = ax1.twinx() \n","    ax2.set_ylabel(\"Cost function\")\n","    ax2.plot(x,y3, label=\"Loss\")\n","\n","    ax1.legend(loc=\"upper right\",  prop={'size': 15}, bbox_to_anchor=(1, 0.9))\n","    ax2.legend(loc=\"upper right\", prop={'size':15})\n","\n","    string_score =\"Max MAPS = {:.4f}\\nMax MMR = {:.4f}\\nEpoch = {}\".format(maps[max_score_index],mmrs[max_score_index],max_score_index)\n","    fig.text(position[0],position[1],string_score,bbox={'boxstyle':'square',\"color\":\"white\"},fontdict={'color':'black','size':'15'})\n","    \n","    fig.savefig('ABCNN1-2.png')\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"v8X1x8rR1lBx","colab":{}},"cell_type":"code","source":["cost_file = \"cost/-ABCNN1-2.txt\"\n","score_file = \"experiments/-ABCNN1-2-LR.txt\"\n","position = [0.5,0.75]\n","plt_graphs(cost_file,score_file,position)"],"execution_count":0,"outputs":[]}]}